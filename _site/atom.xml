<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>annotatAR</title>
 <link href="http://arebe.github.io/annotatar/atom.xml" rel="self"/>
 <link href="http://arebe.github.io/annotatar/"/>
 <updated>2015-11-02T18:12:24-05:00</updated>
 <id>http://arebe.github.io</id>
 <author>
   <name>RB</name>
   <email>rfboyce+annotatar@gmail.com</email>
 </author>

 
 <entry>
   <title>Week 9 Delinquncy</title>
   <link href="http://arebe.github.io/annotatar/2015/11/02/week9/"/>
   <updated>2015-11-02T00:00:00-05:00</updated>
   <id>http://arebe.github.io/2015/11/02/week9</id>
   <content type="html">&lt;p&gt;Overthinking and under-doing in October. Next steps.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;how-i-spent-october&quot;&gt;How I spent October&lt;/h4&gt;

&lt;p&gt;I’ve had a very fruitful October. Unfortunately, this activity has not been focused on my own projects and thus annotatAR has been left on the wayside for far too long.&lt;/p&gt;

&lt;p&gt;While I’ve been a bit delinquent in updating both this project blog and working on the project itself, I have accomplished much rumination on the final outcome for this project. &lt;/p&gt;

&lt;p&gt;I somewhat switched focus at the begining of October, and began to think of the project as a sort of meta-documentation tool whose primary end-user would be me. This focus hasn’t sat well with me though, as the spirit of the project has been to build experientially what has already been ongoing virtually.&lt;/p&gt;

&lt;p&gt;Particular questions remain to be answered:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Who is the audience for the mobile site? &lt;/li&gt;
  &lt;li&gt;In what context will they discover it?&lt;/li&gt;
  &lt;li&gt;Will the site display current or historic tweets?&lt;/li&gt;
  &lt;li&gt;What controls will the end user have over the tweets being display?&lt;/li&gt;
  &lt;li&gt;Will the website generate a database of tweet data?&lt;/li&gt;
  &lt;li&gt;Will multiple hashtags be available at a particular location?&lt;/li&gt;
  &lt;li&gt;Will the end user be able to change tweets?&lt;/li&gt;
  &lt;li&gt;Will there be an administrative site where an admin can set up a new database entry for a geolocation / hashtag pairing?&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;audience&quot;&gt;Audience&lt;/h4&gt;

&lt;p&gt;My current thinking on the purpose of annotatAR is to fully embrace the art project aspect. Rather than thinking that this project is a tool to make other works, I would like to present it as a work in itself, a generative social media artwork. &lt;/p&gt;

&lt;p&gt;In this case, the audience / end user becomes those interested in radical use of digital spaces, Twitter and social media, and contemporary activist movements.&lt;/p&gt;

&lt;p&gt;The limitations of this direction are that the promary purpose is no longer documentation of an event in process. The Twitter stream API will likely not be useful - it will be necessary to utilize historic tweet data, such as has been gathered by Occupy Research.&lt;/p&gt;

&lt;h4 id=&quot;context&quot;&gt;Context&lt;/h4&gt;

&lt;p&gt;The working context for annotatAR will be a digital exhibition of some kind, where the audience finds out about the work through some third party - much like the Invisible Monument project. The artwork will be available for a specified timeframe during which someone can go to the appropriate location and experience the augmented reality app on their Android mobile device. &lt;/p&gt;

&lt;p&gt;This work might be highlighted in a launch event or tour of some kind. Once annotatAR has a first version, I can begin to look at fellowships and opportunities to take the project further and share it with communities.&lt;/p&gt;

&lt;h4 id=&quot;stream-vs-rest-vs-historic&quot;&gt;Stream vs REST vs Historic&lt;/h4&gt;

&lt;p&gt;The Twitter API has two versions: the stream which affords access to real-time data as it is posted - and the REST version that allows queries based on various parameters.&lt;/p&gt;

&lt;p&gt;For the conceptual purposes of this project, the stream is ideal, as it expresses the contemporality of dual sites:  online and in the “juice” of physical space. However, the stream depends highly the volume of tweets being generated in a given timeframe; if the event is generating a low level of interest on Twitter, than annotatAR will have sparse data to work with and be less effective as a visual presentation. &lt;/p&gt;

&lt;p&gt;My deployment at the Invisible Monument launch utilized a blend of REST and stream in order to overcome the limitations of the stream API. &lt;/p&gt;

&lt;p&gt;As the purpose of annotatAR has solidified around an experiential artwork, it becomes imperative to use Twitter API in a way that best expresses concept of the piece. The idea of synesthesia becomes an important concept - as we walk through the juice, the ideas and social sites of Twitter are invisible but present. annotatAR interacts with the visual sense to encode the virtual component of a multi-sited co-occurring event.&lt;/p&gt;

&lt;p&gt;The use of historic data, where it exists, affords a slightly different expression of this co-occurrence. Instead of expressing contemporality, the visual metaphor would be more archeological in tone - that of unearthing and re-experiencing an event in the past, like an historical marker expressed in virtuality.&lt;/p&gt;

&lt;h4 id=&quot;hashtag--geolocation&quot;&gt;Hashtag / Geolocation&lt;/h4&gt;

&lt;p&gt;Should multiple hashtags be associated with a specific geolocation? &lt;/p&gt;

&lt;p&gt;So doing might dilute the strength of the connection between a juice space and a twitter hashtag settlement - but it would also afford a more nuanced understanding of historicity of a space.&lt;/p&gt;

&lt;h4 id=&quot;end-user-controls&quot;&gt;End user controls&lt;/h4&gt;

&lt;p&gt;Given the above discussion, the end user will have some minimal controls - to select a hashtag (if there are multiple available at the geolocation), to pan around, to take a screenshot, and to scroll through time.&lt;/p&gt;

&lt;h4 id=&quot;desktop-site&quot;&gt;Desktop site&lt;/h4&gt;

&lt;p&gt;The desktop site will be quite minimal - a brief description of the project and map of the geolocations available. A sign-up for newsletter of project updates. &lt;/p&gt;

&lt;h4 id=&quot;questions&quot;&gt;Questions&lt;/h4&gt;

&lt;p&gt;What historic tweet data is available? - this needs to be answered ASAP to properly scope this and future versions.&lt;/p&gt;

&lt;p&gt;Future directions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Funding for a tweet aggregation service such as GNIP&lt;/li&gt;
  &lt;li&gt;Development of a robust administrative layer&lt;/li&gt;
  &lt;li&gt;Development of a way to share screen captures &lt;/li&gt;
  &lt;li&gt;Integration with the Invisible Monument project (iOS)&lt;/li&gt;
  &lt;li&gt;Development of a robust data collection system to gather data on contemporary events&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;More immediate goals for annotatAR (still) include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Bug fixing - hashtags on the stream seemed not to work on the website (though the stream worked on the local version)&lt;/li&gt;
  &lt;li&gt;Fluid accelerometer use&lt;/li&gt;
  &lt;li&gt;URL routing for hashtag display&lt;/li&gt;
  &lt;li&gt;Automation or parameterization of annotatAR UX elements (ie age til die-off)&lt;/li&gt;
  &lt;li&gt;Capturing additional tweet data (ie user, associated photos) for posterity&lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Week 5 Invisible Monument</title>
   <link href="http://arebe.github.io/annotatar/2015/10/02/week5/"/>
   <updated>2015-10-02T00:00:00-04:00</updated>
   <id>http://arebe.github.io/2015/10/02/week5</id>
   <content type="html">&lt;p&gt;Outcomes from deployement at Invisible Monument launch event in Boston.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;lead-up-to-deployment&quot;&gt;Lead up to deployment&lt;/h4&gt;

&lt;p&gt;In the past week, I had a development push to meet the development goals that were most important to be able to demonstrate annotatAR’s utility as a docmentation tool. I met those goals for the most part. I was able to implement:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Typography and color changes for better legibilty&lt;/li&gt;
  &lt;li&gt;Stripping the tweet text of URLs&lt;/li&gt;
  &lt;li&gt;Screen capture button to save an image of the &lt;code&gt;canvas&lt;/code&gt; element&lt;/li&gt;
  &lt;li&gt;Panning with accelerometer data - incomplete implementation&lt;/li&gt;
  &lt;li&gt;Integrating stream and REST Twitter API data&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Ideally, I want to use accelerometer data from the device to allow the end user to pan around - to change the position of the tweets to create the illusion that they are positioned in three dimensional space. While I wasn’t able to achieve a reliable 3D effect, I started to experiment with the &lt;code&gt;ondevicemotion&lt;/code&gt; JavaScript event, and &lt;code&gt;accellerationIncludingGravity&lt;/code&gt; on the X and Y axes. While I was able to achieve somewhat fluid UX on my laptop, the calibration did not translate properly to my Galaxy 4G device.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/annotatar/assets/images/20151002_invisible/invisible_screenshot_20150930_00.png&quot; alt=&quot;annotatAR at #LuncheonOnTheGrass&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;invisible-monument&quot;&gt;Invisible Monument&lt;/h4&gt;

&lt;p&gt;The Invisible Monument launch event, #LuncheonOnTheGrass took place on 30 Sept 2015, in Dewey Square, Boston. &lt;a href=&quot;http://invisiblemonument.com/&quot;&gt;Invisible Monument&lt;/a&gt; is a locative sound installation created by Halsey Burgund and Lara Baladi, that cn be experienced using an iOS application. Various sond elements are triggered by geolocations around Dewey Square. &lt;/p&gt;

&lt;p&gt;I met the team on site for the launch event and started taking screen captures with annotatAR. I used three different Twitter tags: #occupy, #the99, and #luncheononthegrass. I adjusted the “die-off” rate for each of the tweets according to how often the tag was used. As the tweets get older, they become smaller and more transparent. &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/annotatar/assets/images/20151002_invisible/invisible_screenshot_20150930_01.png&quot; alt=&quot;annotatAR at #LuncheonOnTheGrass&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/annotatar/assets/images/20151002_invisible/invisible_screenshot_20150930_02.png&quot; alt=&quot;annotatAR at #LuncheonOnTheGrass&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/annotatar/assets/images/20151002_invisible/invisible_screenshot_20150930_03.png&quot; alt=&quot;annotatAR at #LuncheonOnTheGrass&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/annotatar/assets/images/20151002_invisible/invisible_screenshot_20150930_04.png&quot; alt=&quot;annotatAR at #LuncheonOnTheGrass&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/annotatar/assets/images/20151002_invisible/invisible_screenshot_20150930_05.png&quot; alt=&quot;annotatAR at #LuncheonOnTheGrass&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/annotatar/assets/images/20151002_invisible/invisible_screenshot_20150930_06.png&quot; alt=&quot;annotatAR at #LuncheonOnTheGrass&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/annotatar/assets/images/20151002_invisible/invisible_screenshot_20150930_07.png&quot; alt=&quot;annotatAR at #LuncheonOnTheGrass&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/annotatar/assets/images/20151002_invisible/invisible_screenshot_20150930_08.png&quot; alt=&quot;annotatAR at #LuncheonOnTheGrass&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;historic-tweets&quot;&gt;Historic tweets&lt;/h4&gt;

&lt;p&gt;I would like to be able to overlay historic tweet data as well as current tweets. However, the &lt;a href=&quot;https://dev.twitter.com/rest/public/search&quot;&gt;Twitter REST API&lt;/a&gt; allows access to tweets created within the past couple of weeks only.&lt;/p&gt;

&lt;p&gt;Third-party services such as &lt;a href=&quot;https://gnip.com/&quot;&gt;GNIP&lt;/a&gt; are social media brokers from which you can purchase historic data. The base rate for historic tweet data starts at $1.1k and increased depending on length of time and number of tweets returned by the query. &lt;/p&gt;

&lt;p&gt;While this is cost-prohibitive for the time being, it does add a layer of utility to annotatAR. I have already implemented the ability to download the database of tweets as a CSV file, which could be quite useful in capturing an independent record of a hashtag.&lt;/p&gt;

&lt;p&gt;Additionally, I spoke to one of the participants in #LuncheonOnTheGrass who directed me to an archive of tweets from the Occupy movement, available at &lt;a href=&quot;http://occupyresearch.net/&quot;&gt;Occupy Research&lt;/a&gt; and &lt;a href=&quot;http://r-shief.org/&quot;&gt;R-Shief&lt;/a&gt;. These might be a useful corpus to draw on in future iterations. &lt;/p&gt;

&lt;h4 id=&quot;future-of-invisible-monument&quot;&gt;Future of Invisible Monument&lt;/h4&gt;

&lt;p&gt;Lara and team were quite excited by the output from annotatAR, and will use my documentation of #LuncheonOnTheGrass as part of the ongoing archive for Invisible Monument / Vox Populi. &lt;/p&gt;

&lt;p&gt;We began to discuss how annotatAR might be integrated with Halsey’s &lt;a href=&quot;http://www.roundware.org/&quot;&gt;Roundware&lt;/a&gt; platform, to provide an integrated experience with the audioscape. Future directions for Invisible Monument are quite exciting, and I hope to continue to work with them. However, I think this will only be possible of Halsey ports Roundware to Android or the mobile web.&lt;/p&gt;

&lt;h4 id=&quot;near-term-goals&quot;&gt;Near term goals&lt;/h4&gt;

&lt;p&gt;More immediate goals for annotatAR include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Bug fixing - hashtags on the stream seemed not to work on the website (though the stream worked on the local version)&lt;/li&gt;
  &lt;li&gt;Bug fixing - houston doesn’t seem to work, maybe due to special characters?&lt;/li&gt;
  &lt;li&gt;Fluid accelerometer use&lt;/li&gt;
  &lt;li&gt;URL routing for hashtag display&lt;/li&gt;
  &lt;li&gt;Automation or parameterization of annotatAR UX elements (ie age til die-off)&lt;/li&gt;
  &lt;li&gt;Capturing additional tweet data (ie user, associated photos) for posterity&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Once these goals are achieved it may be time to turn to text analysis and a desktop site.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Week 4 Scope and Usability</title>
   <link href="http://arebe.github.io/annotatar/2015/09/23/week4/"/>
   <updated>2015-09-23T00:00:00-04:00</updated>
   <id>http://arebe.github.io/2015/09/23/week4</id>
   <content type="html">&lt;p&gt;Revisiting the project goals, scope, and affordances.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;reorienting&quot;&gt;Reorienting&lt;/h4&gt;

&lt;p&gt;Last Fall, I has the opportunity to work with the talented &lt;a href=&quot;http://docshop.space&quot;&gt;DocShop&lt;/a&gt; team on an experimental event called &lt;a href=&quot;http://docshop0.tumblr.com/&quot;&gt;Notes from El Saniyya&lt;/a&gt; in which we worked with artist Lara Baladi to prototype her ongoing &lt;a href=&quot;http://www.tahrirarchives.com/&quot;&gt;Vox Populi&lt;/a&gt; project. This ambitious work is an interactive multimedia archive of the Arab Spring, specifically the Egyptian Revolution of 2011. The experience of working with archival material in a socially-aware setting has been a deep inspiration for annotatAR and its design goals.&lt;/p&gt;

&lt;iframe src=&quot;https://player.vimeo.com/video/132129973?title=0&amp;amp;byline=0&amp;amp;portrait=0&quot; width=&quot;600&quot; height=&quot;337&quot; frameborder=&quot;0&quot; webkitallowfullscreen=&quot;&quot; mozallowfullscreen=&quot;&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;p&gt;&lt;a href=&quot;https://vimeo.com/132129973&quot;&gt;Vox Populi, Archiving a Revolution in the Digital Age&lt;/a&gt; from &lt;a href=&quot;https://vimeo.com/user41572927&quot;&gt;Lara Baladi&lt;/a&gt; on &lt;a href=&quot;https://vimeo.com&quot;&gt;Vimeo&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It was with great excitement that I received an email from Lara last week, with news of the next iteration of the project: a picnic in Dewey Square to commemorate the anniversary of &lt;a href=&quot;http://www.occupyboston.org/&quot;&gt;Occupy Boston&lt;/a&gt;, a collective protest and re-settlement of public space that occurred from September 30 - December 10, 2011 in solidarity with &lt;a href=&quot;http://occupywallst.org/&quot;&gt;Occupy Wall Street&lt;/a&gt; and other &lt;a href=&quot;https://en.wikipedia.org/wiki/Occupy_movement&quot;&gt;Occupy movements&lt;/a&gt; around the world.&lt;/p&gt;

&lt;p&gt;Lara is excited to utilized annotatAR as part of the commemorative picnic, and we are in the process of hashing out the details for incorporating my software. However, the overall goals and aesthetic of her event require some re-thinking of how to best to incorporate annotatAR as a digital tool. We’ve agreed that at this point it would be best to use annotatAR as a documentation tool rather than deploying it as a full-fledged augmented reality experience. &lt;/p&gt;

&lt;h4 id=&quot;goals-for-the-picnic&quot;&gt;Goals for the picnic&lt;/h4&gt;

&lt;p&gt;September 30 is approaching rapidly, and I’d really like to be able to utilize annotatAR to the fullest extent. My experience at the Internet Yami-Ichi was that, while the mechanics worked, there are many aesthetic improvements to be made.&lt;/p&gt;

&lt;p&gt;New focal points include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Legibility of the tweet text through typography and color palette&lt;/li&gt;
  &lt;li&gt;Resolution of video&lt;/li&gt;
  &lt;li&gt;Screen capture &lt;/li&gt;
  &lt;li&gt;Switching between several hashtags &lt;/li&gt;
  &lt;li&gt;Displaying tweets from 2011 to capture the historicity of the current event&lt;/li&gt;
  &lt;li&gt;Filtering out URLs that clutter the image with illegible content&lt;/li&gt;
  &lt;li&gt;Locating the tweets in a 3D space to further spread them out and increase legibility&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;post-picnic-content&quot;&gt;Post-picnic content&lt;/h4&gt;

&lt;p&gt;Once we have screenshots created with annotatAR the next step is to generate an expressive media object using these images. Perhaps this takes the form of a multi-media collage. &lt;/p&gt;

&lt;p&gt;I’d like to incorporate text visualization from the tweets for a multi-modal presentation, as an interactive website along with static images and video. The end-user has become the archival artist (such as myself and Lara) and ultimately the general public. In this case success can be assessed by determining reactions of the audience, if the work elicits a sense of historicity and participation, and an emotive reaction. I believe this work also helps me process my experience of participating in Occupy, and commemorating it in an official capacity is an opportunity to orient my own experiences within a global context. My goal for this project is to do the same for other participants in Occupy - as well as provide a touchstone for folks who are interested in contemporary protest movements.&lt;/p&gt;

&lt;h4 id=&quot;usability-and-scope&quot;&gt;Usability and scope&lt;/h4&gt;

&lt;p&gt;This new orientation for annotatAR focuses more on the output of screenshots than usability in the sense of affordances to a naive end-user. Geolocation is unnecessary for this iteration and cross-browser and -platform compatibility become irrelevant.&lt;/p&gt;

&lt;p&gt;Narrowing the scope of the project to a known and tested platform (Firefox on a Galaxy 4G smart phone) makes the project more manageable and provides a clear direction for design decisions. Geolocative and “on-boarding” affordances of the UI are features that can wait until the visual design has been refined.&lt;/p&gt;

&lt;h4 id=&quot;further-planning&quot;&gt;Further planning&lt;/h4&gt;

&lt;p&gt;As I continue to work with Lara and the DocShop team to prepare for the event, I believe more design ideas will surface. I expect that the next week will be full of challenges, brainstorming, and new ideas.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Week 3 Beta Testing at NYC Internet Yami-Ichi</title>
   <link href="http://arebe.github.io/annotatar/2015/09/14/week3a/"/>
   <updated>2015-09-14T00:00:00-04:00</updated>
   <id>http://arebe.github.io/2015/09/14/week3a</id>
   <content type="html">&lt;p&gt;In which we visit the internet black market and take some screenshots.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&quot;/annotatar/assets/images/20150912_yaminyc/brian_esser_yaminyc.jpg&quot; alt=&quot;NYC Internet Yami-Ichi (photo credit to Brian Esser)&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;preparation&quot;&gt;Preparation&lt;/h4&gt;

&lt;p&gt;I was tipped off to the existence of the &lt;a href=&quot;http://yami-ichi.biz/nyc/&quot;&gt;NYC Internet Yami-Ichi&lt;/a&gt; by some colleagues from &lt;a href=&quot;http://itp.nyu.edu/camp2015/&quot;&gt;ITP Camp at NYU&lt;/a&gt;. The event began in Japan and is devoted to the extrapolation of everything “internet-ish” into the real world. Due to my interest in net art and utilizing online interactions as an artistic medium, I was immediately interested in participating. It also seemed like an excellent place for the first test of annotatAR in the wild.&lt;/p&gt;

&lt;p&gt;I set the bar for the beta-test fairly low - the key here would be to see how and if people utilized the core functionality of tweets displayed on a real-time &lt;code&gt;getUserMedia&lt;/code&gt; video feed. I deployed a test version of the app on Meteor’s test server. The Meteor platform proved itself useful in this regard: it was quite easy to deploy multiple versions from the same code base by simply changing the URL in the &lt;code&gt;meteor deploy&lt;/code&gt; command.&lt;/p&gt;

&lt;p&gt;annotatAR achieved the minimal viable test build plus a few extra features:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;geolocation (coordinates and accuracy) was displayed below the video feed - for testing purposes only&lt;/li&gt;
  &lt;li&gt;tweet data was displayed in decresing font size - the age of the tweet was linearly mapped to the font scale - tweets appeared as 50px in size and decreased down to 5px over the course of 8 hours&lt;/li&gt;
  &lt;li&gt;the databse stores the hashtag for the search that generates each the tweet&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;survey&quot;&gt;Survey&lt;/h4&gt;

&lt;p&gt;I also designed a brief user feedback survey and posted it on Google Forms. I also made a paper version (which is what I ended up utiliting in situ).&lt;/p&gt;

&lt;p&gt;The survey captures whether or not the video and tweet functions work, information about th edevice &amp;amp; browser, and some open-ended questions.&lt;/p&gt;

&lt;h4 id=&quot;internet-yami-ichi-deployment&quot;&gt;Internet Yami-Ichi Deployment&lt;/h4&gt;

&lt;p&gt;The &lt;a href=&quot;http://annotatar_yaminyc.meteor.com&quot;&gt;#yaminyc app&lt;/a&gt; was deployed the night before the event, and targeted at the Twitter hashtag “#yaminyc” where a couple of folks had already begun to interact. &lt;/p&gt;

&lt;p&gt;I acheived two separate user tests during the event. One was successful and generated some great feedback regarding next directions for the UI: a “selfie” mode, screencapture interface button, and an animated GIF generator. The second test was not as successful - it was an iPhone and it seemed that the phone did not ask the user for permission to connect to the &lt;code&gt;getUserMedia&lt;/code&gt; device at all. This may be a significant issue going forward, if iPhone defaults interfere with this browser funcitonality.&lt;/p&gt;

&lt;p&gt;I also took a number of screenshots at the event. It is quite an expressive way of documenting this type of gathering - one gets a sense of a crowd of virtual eavesdropping entities. &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/annotatar/assets/images/20150912_yaminyc/annotatar_yaminyc_20150912_00.png&quot; alt=&quot;annotatAR at NYC Internet Yami-Ichi&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/annotatar/assets/images/20150912_yaminyc/annotatar_yaminyc_20150912_01.png&quot; alt=&quot;annotatAR at NYC Internet Yami-Ichi&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/annotatar/assets/images/20150912_yaminyc/annotatar_yaminyc_20150912_02.png&quot; alt=&quot;annotatAR at NYC Internet Yami-Ichi&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/annotatar/assets/images/20150912_yaminyc/annotatar_yaminyc_20150912_03.png&quot; alt=&quot;annotatAR at NYC Internet Yami-Ichi&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/annotatar/assets/images/20150912_yaminyc/annotatar_yaminyc_20150912_04.png&quot; alt=&quot;annotatAR at NYC Internet Yami-Ichi&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/annotatar/assets/images/20150912_yaminyc/annotatar_yaminyc_20150912_05.png&quot; alt=&quot;annotatAR at NYC Internet Yami-Ichi&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/annotatar/assets/images/20150912_yaminyc/annotatar_yaminyc_20150912_06.png&quot; alt=&quot;annotatAR at NYC Internet Yami-Ichi&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/annotatar/assets/images/20150912_yaminyc/annotatar_yaminyc_20150912_07.png&quot; alt=&quot;annotatAR at NYC Internet Yami-Ichi&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/annotatar/assets/images/20150912_yaminyc/annotatar_yaminyc_20150912_08.png&quot; alt=&quot;annotatAR at NYC Internet Yami-Ichi&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/annotatar/assets/images/20150912_yaminyc/annotatar_yaminyc_20150912_09.png&quot; alt=&quot;annotatAR at NYC Internet Yami-Ichi&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/annotatar/assets/images/20150912_yaminyc/annotatar_yaminyc_20150912_10.png&quot; alt=&quot;annotatAR at NYC Internet Yami-Ichi&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/annotatar/assets/images/20150912_yaminyc/annotatar_yaminyc_20150912_11.png&quot; alt=&quot;annotatAR at NYC Internet Yami-Ichi&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;lessons&quot;&gt;Lessons&lt;/h4&gt;

&lt;p&gt;I think that annotatAR was quite effective in achieving the union of virtual and real space in an expressive way. &lt;/p&gt;

&lt;p&gt;I was a bit frustrated with the quality of the web camera picture, I wonder if anything can be done to improve it.&lt;/p&gt;

&lt;p&gt;I’m also brainstorming about the next level of the project: the desktop site. Perhaps it should be a focal point for sharing screenshots, or perhaps the screenshots could be utilized in a larger work of some kind. I now have a databse of tweets from an event to experiment with, as a corpus for text analysis.&lt;/p&gt;

&lt;p&gt;I’m a bit concerned about the &lt;a href=&quot;http://iswebrtcreadyyet.com/&quot;&gt;failure of iOS&lt;/a&gt; to connect to the WebRTC protocol, which I was not expecting since it is supported by the versions of Firefox and Chrome that are on the iPhone device thtat was tested. I will address this in my upcoming post on scoping for compatibility and audience. &lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Week 2 Feedback and Learning</title>
   <link href="http://arebe.github.io/annotatar/2015/09/13/week2/"/>
   <updated>2015-09-13T00:00:00-04:00</updated>
   <id>http://arebe.github.io/2015/09/13/week2</id>
   <content type="html">&lt;p&gt;In which annotatAR undergoes critique by two fellow capstone students, RB converses with Jen Kramer, and preparation is underway for the first beta deployment.&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;check-in-with-jen&quot;&gt;Check-in with Jen&lt;/h4&gt;

&lt;p&gt;Call with Jen K was super helpful in thinking about next steps for the design and how to justify decisions with demographics and user personas. It was great to have a chat about high-level project goals, and it was super helpful start to think about deliverables in terms of the December presentations. &lt;/p&gt;

&lt;p&gt;We reviewed the rubric and I’m going to revisit it in light of our conversation; the main advice was to try to define the criteria as specifically as possible so we are on the same page about the final deliverables. I will spend s bit of time thinking through the aesthetic qualities that are most important. Similarly, I will work to define a specific use-case for device compatibility and narrow the scope to what makes the most sense for the target audience demographic.&lt;/p&gt;

&lt;h4 id=&quot;feedback-from-pat--peg&quot;&gt;Feedback from Pat &amp;amp; Peg&lt;/h4&gt;

&lt;p&gt;My group members provided copius and helpful feedback - I think it really helped that I asked a couple of specific but somewhat open-ended questions in my video :) The main areas where I hoped to receive suggestions were user experience and scaling the project down the road. Overall, I’m hearing that a simple interface for the mobile site is preferable - the tweets should be obvious and contrasting. Scaling could go in various directions - Pat had some suggestions about devices which I will consider when I revisit my grading rubric. I look forward to incorporating the thoughtful ideas that Peg and Pat have shared with me.&lt;/p&gt;

&lt;h4 id=&quot;learning-meteor&quot;&gt;Learning Meteor&lt;/h4&gt;

&lt;p&gt;I went to a Meteor meet-up in New York and met some awesome Meteor folks. I learned about &lt;a href=&quot;https://www.discovermeteor.com/&quot;&gt;Discover Meteor&lt;/a&gt;, the guide to getting started. I also learned about how Meteor handles authorization and authentication, why you might want to render views on the server (which is functionality that will part of the next release of Meteor), and also heard about the Meteor package &lt;a href=&quot;https://atmospherejs.com/houston/admin&quot;&gt;houston&lt;/a&gt; for implementing a database UI. &lt;/p&gt;

&lt;p&gt;In preparing for the beta deployment at the &lt;a href=&quot;http://yami-ichi.biz/nyc/&quot;&gt;Internet Yami-Ichi NYC&lt;/a&gt;, I realized that an HTML5 game engine might be a suitable platform for displaying the tweet database. Such a mobile framework wold handle the canvas on the mobile app - something as simple as resizing the video to fit the screen seems needlessly complex to code from scratch. So I’m seeking a mobile-first render engine. Some candidates:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.pixijs.com/&quot;&gt;Pixi.js&lt;/a&gt; - a 2D webgl renderer&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://phaser.io&quot;&gt;Phaser&lt;/a&gt; - HTML 5 game framework that’s (previously) based on Pixi&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://famous.org/&quot;&gt;Famous&lt;/a&gt; - js library for animations &amp;amp; interfaces (not as much doc)&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;looking-ahead-this-week&quot;&gt;Looking ahead this week&lt;/h4&gt;

&lt;p&gt;The upcoming week, I will develop a user persona based on the demographics of twitter users, sketch out the app database design, and mock up the UI for the mobile site.&lt;/p&gt;

&lt;p&gt;I’ll also be posting a recap of this weekend’s beta test as its own post.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Week 1 Summer Progress</title>
   <link href="http://arebe.github.io/annotatar/2015/09/04/week1/"/>
   <updated>2015-09-04T00:00:00-04:00</updated>
   <id>http://arebe.github.io/2015/09/04/week1</id>
   <content type="html">&lt;p&gt;In which our hero recounts the details of this summer’s work….&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;user-interaction-paradigm&quot;&gt;User interaction paradigm&lt;/h4&gt;

&lt;p&gt;annotatAR was inspired by a project I worked on in Fall 2014, &lt;a href=&quot;http://docshop0.tumblr.com/&quot;&gt;Notes from El Saniyya&lt;/a&gt;. As part of &lt;a href=&quot;http://docshop.space/&quot;&gt;DocShop at metaLab&lt;/a&gt;, I worked with Egyptian artist Lara Baladi to create a prototype of a living timeline of the Arab Spring. The &lt;em&gt;Notes…&lt;/em&gt; event was highly participatory and we wondered what the next steps for engaging with a multi-media archive might be like.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;You walk into a gallery space, with a curved wall with a huge timeline, each point on the timeline is an event in the Arab Spring / Tahrir Square 21 days. you want to add your own comment to the timeline, reacting to something on the timeline that triggered a memory or emotion - given any possible technology or material, how would you leave a message?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is motivation for utilizing an augmented reality platform, and Twitter as a common social interface.&lt;/p&gt;

&lt;h4 id=&quot;meteor&quot;&gt;Meteor&lt;/h4&gt;

&lt;p&gt;The major accomplishment achieved this summer was getting started with &lt;a href=&quot;http://docs.meteor.com/#/full/&quot;&gt;Meteor.js&lt;/a&gt;. After reading docs and going through an example project tutorial, I was convinced that this was the technology on which I should build annotatAR. What first drew me to Meteor is the ability to code everything in Javascript, the active community, and the well-designed forward-looking codebase. I sat with a friend for an informational session to learn about how to utilize &lt;a href=&quot;https://www.eventedmind.com/feed/meteor-what-is-meteor-bindenvironment&quot;&gt;asynchronous external requests&lt;/a&gt; in Meteor, through &lt;a href=&quot;https://meteorhacks.com/fibers-eventloop-and-meteor&quot;&gt;fibers&lt;/a&gt; and &lt;code&gt;Meteor.bindEnvironment&lt;/code&gt;. I’m starting to see even more value in how Meteor encourages code modularization, which may be quite helpful as the project develops.&lt;/p&gt;

&lt;h4 id=&quot;alpha-site&quot;&gt;Alpha site&lt;/h4&gt;

&lt;p&gt;With my newly acquired knowledge of Meteor, I built a test application called &lt;em&gt;truthtweets&lt;/em&gt;, which connects to the &lt;a href=&quot;https://dev.twitter.com/streaming/public&quot;&gt;Twitter Firehose API&lt;/a&gt; and overlays tweet texts that utilize the hashtag &lt;em&gt;#truth&lt;/em&gt; onto a video stream from &lt;code&gt;getUserMedia&lt;/code&gt; element. Voilà!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/annotatar/assets/images/screencap_20150904.png&quot; alt=&quot;truthtweets app&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;next-directions&quot;&gt;Next directions&lt;/h4&gt;

&lt;p&gt;In the short term, the mobile application has a number of refinements to be achieved:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;strike&gt;twitter api config file - untracked (not shared on github)&lt;/strike&gt;
    &lt;p&gt;done! - now using settings.json&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;resize navigator to fit device screen&lt;/li&gt;
  &lt;li&gt;use accelerometer data to detect motion and update tweet position&lt;/li&gt;
  &lt;li&gt;turn the stream on and display new tweets&lt;/li&gt;
  &lt;li&gt;change alpha based on tweet createdAt&lt;/li&gt;
  &lt;li&gt;deploy to meteor cloud or other web server&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;what if tweets became colored to reflect the background video feed? - lexigraph&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;looking-ahead-this-month&quot;&gt;Looking ahead this month&lt;/h4&gt;

&lt;p&gt;My first &lt;a href=&quot;/annotatar/2015/09/02/milestones&quot;&gt;project milestone&lt;/a&gt; will be next week, when I deploy a beta version of the application on a live server and test it at the &lt;a href=&quot;http://yami-ichi.biz/nyc/&quot;&gt;New York City Internet Yami-Ichi&lt;/a&gt; event by giving the URL out to a (small) number of attendees. This early-stage test will be crucial in refining the user interactions and aesthetics. In addition to the application itself, I will design a brief Google forms survey to collect UX feedback.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Capstone Project Grading Rubric</title>
   <link href="http://arebe.github.io/annotatar/2015/09/03/grading/"/>
   <updated>2015-09-03T00:00:00-04:00</updated>
   <id>http://arebe.github.io/2015/09/03/grading</id>
   <content type="html">&lt;p&gt;As part of this week’s assignments, I have developed a grading rubric for annotatAR, for the scope of this fall’s Capstone Design Studio. This rubric has six dimensions, with three criteria each. The dimensions are categorized in two sub-sections: &lt;em&gt;Mobile Site&lt;/em&gt; and &lt;em&gt;Desktop Site&lt;/em&gt;. The points are weighted to emphasize the &lt;em&gt;Mobile Site&lt;/em&gt;, with a total possible score of 27 points.&lt;/p&gt;

&lt;hr /&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;em&gt;Mobile Site (18pts total)&lt;/em&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Excellent (6pts)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Competent (4pts)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Needs Work (1pt)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Functionality&lt;/strong&gt; :: Users can overlay tweets on a video stream; Device compatibility;  Geolocative;  Users can take screenshot&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Meets all requirements&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Meets 3/4 requirements&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Meets &amp;lt;= 3 requirements&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Aesthetics&lt;/strong&gt; :: Tweet metadata is visually encoded; Tweet position changes with device motion; Appealing visual design&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Meets all requirements&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Meets 2/3 requirements&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Meets &amp;lt;= 2 requirements&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Scalability&lt;/strong&gt; :: Deployable at a variety of events; Github repo + documentation for further development&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Meets all requirements&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Meets 1/2 requirements&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Meets &amp;lt; 1 requirement&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;em&gt;Desktop Site (9pts total)&lt;/em&gt;&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Excellent (3pts)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Competent (2pts)&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Needs Work (1pt)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Functionality&lt;/strong&gt; :: Users can view event-specific video with tweet overlay; Cross-browser compatibility&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Meets all requirements&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Meets 1/2 requirements&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Meets &amp;lt; 1 requirement&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Aesthetics&lt;/strong&gt; :: Tweet metadata is visually encoded; Timeline of event is expressed visually&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Meets all requirements&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Meets 1/2 requirements&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Meets &amp;lt; 1 requirement&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Scalability&lt;/strong&gt; :: Deployable for multiple events; Integrates with other social media feeds&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Meets all requirements&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Meets 1/2 requirements&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Meets &amp;lt; 1 requirement&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
</content>
 </entry>
 
 <entry>
   <title>Capstone Project Milestones</title>
   <link href="http://arebe.github.io/annotatar/2015/09/02/milestones/"/>
   <updated>2015-09-02T00:00:00-04:00</updated>
   <id>http://arebe.github.io/2015/09/02/milestones</id>
   <content type="html">&lt;p&gt;As part of this week’s set of assignments, I have revisited the project milestones for annotatAR this fall. &lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;beta-test-at-the-nyc-internet-yami-ichihttpyami-ichibiznyc&quot;&gt;Beta-test at the &lt;a href=&quot;http://yami-ichi.biz/nyc/&quot;&gt;NYC Internet Yami-Ichi&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;12 Sept 2015&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Mobile site deployed on a live server&lt;/li&gt;
  &lt;li&gt;Displays tweet stream overlaid on &lt;code&gt;getUserMedia&lt;/code&gt; element&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;device-sensing&quot;&gt;Device sensing&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;30 Sept 2015&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Geolocation-aware&lt;/li&gt;
  &lt;li&gt;Access device accelerometer data&lt;/li&gt;
  &lt;li&gt;Map tweet position to 3D model&lt;/li&gt;
  &lt;li&gt;Determine whether or not to compile to native app(s)&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;aesthetic-refinements-on-mobile&quot;&gt;Aesthetic refinements on mobile&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;16 Oct 2015&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Style tweets based on timestamp&lt;/li&gt;
  &lt;li&gt;Screenshot functionality&lt;/li&gt;
  &lt;li&gt;User testing of mobile&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;desktop-site-minimum-viable-product&quot;&gt;Desktop site minimum viable product&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;30 Oct 2015&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Display video from event with tweets overlaid&lt;/li&gt;
  &lt;li&gt;User testing of desktop&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;desktop-site-refinements&quot;&gt;Desktop site refinements&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;20 Nov 2015&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Text analysis (some or all of the following):
    &lt;ul&gt;
      &lt;li&gt;Sentiment analysis&lt;/li&gt;
      &lt;li&gt;Keyword parsing&lt;/li&gt;
      &lt;li&gt;Spatial and/or color encoding&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Scrub / navigate on time dimension&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;platform-scalability&quot;&gt;Platform scalability&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;04 Dec 2015&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Github repo with description and instructions for deployment&lt;/li&gt;
  &lt;li&gt;Integrate with existing social media (share buttons)&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;bonus-features&quot;&gt;Bonus features&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;11 Dec 2015&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;User interface for deploying new app for an event&lt;/li&gt;
  &lt;li&gt;Compile to native Android app (if necessary)&lt;/li&gt;
  &lt;li&gt;Multiple views of augmented reality:
    &lt;ul&gt;
      &lt;li&gt;Tweet text becomes lexograph of video stream&lt;/li&gt;
      &lt;li&gt;Select variety of encoding for sentiments or keywords&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</content>
 </entry>
 
 <entry>
   <title>Welcome to annotatAR's Blog</title>
   <link href="http://arebe.github.io/annotatar/2015/08/31/welcome/"/>
   <updated>2015-08-31T00:00:00-04:00</updated>
   <id>http://arebe.github.io/2015/08/31/welcome</id>
   <content type="html">&lt;p&gt;This blog will track my progress on my ALM capstone project, annotatAR. Details about the motivation, design, and prospective schedule are part of &lt;a href=&quot;/annotatar/assets/files/ALM_Capstone_Proposal_RBoyce_v3.2.pdf&quot;&gt;the project proposal&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;some-links-to-track&quot;&gt;Some links to track:&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#&quot;&gt;annotatAR live site - coming soon!&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://canvas.harvard.edu/courses/4308&quot;&gt;Capstone design studio site&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;summer-work&quot;&gt;Summer work&lt;/h3&gt;

&lt;p&gt;Over the summer, I learned a bit about &lt;a href=&quot;https://www.meteor.com/&quot;&gt;meteor.js&lt;/a&gt; and began building a version 0 of the site, which connects to the &lt;a href=&quot;https://dev.twitter.com/streaming/public&quot;&gt;Twitter API&lt;/a&gt;, grabs tweets with a particular hashtag, and displays them on top of a video feed from &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Web/API/Navigator/getUserMedia&quot;&gt;getUserMedia&lt;/a&gt;. &lt;/p&gt;

&lt;h3 id=&quot;next-steps&quot;&gt;Next steps&lt;/h3&gt;

&lt;p&gt;While the most basic form of this mechanic “works,” there is much to be done in terms of styling, user interactions, and geolocation. Additionally, meta-analysis of the tweets will add layers of semantics to both the mobile site and eventually to a desktop archival version.&lt;/p&gt;
</content>
 </entry>
 

</feed>
